{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_q_learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOqCt17Z8xHwpKrl8maB9Of",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mftnakrsu/reinforcement-learning/blob/main/deep_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "S2VF66O6dlbW"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import  random\n",
        "from colabgymrender.recorder import  Recorder\n",
        "import tensorflow as tf\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQLAgent:\n",
        "\n",
        "  \n",
        "  def __init__(self,env):\n",
        "    #hyperparameter\n",
        "    self.state_size  =env.observation_space.shape[0]\n",
        "    self.action_size = env.action_space.n\n",
        "\n",
        "    self.gamma  = 0.95\n",
        "    self.l_rate = 0.001\n",
        "\n",
        "    self.epsilon = 1 # baslangıcta her yeri ara\n",
        "    self.epsilon_decay=0.995\n",
        "    self.epsilon_min = 0.01\n",
        "\n",
        "    self.memory=deque(maxlen=1000)\n",
        "\n",
        "    self.opt= tf.keras.optimizers.Adam(lr=self.l_rate)\n",
        "    self.model= self.build_model() # neural network q learning de kullanılan\n",
        "    \n",
        "\n",
        "  def build_model(self):\n",
        "    \"\"\"\n",
        "    neural network forl DQL\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(48,input_dim=self.state_size,activation=\"tanh\"))\n",
        "    model.add(Dense(self.action_size,activation=\"linear\"))\n",
        "    model.compile(loss=\"mse\",optimizer=self.opt)\n",
        "    return model\n",
        "\n",
        "\n",
        "  def remember(self,state,action,reward,next_state,done):\n",
        "    \"\"\"\n",
        "    storage(state action reward done)\n",
        "    \"\"\"\n",
        "    self.memory.append((state,action,reward,next_state,done))\n",
        "\n",
        "\n",
        "  def act(self,state):\n",
        "    \"\"\"\n",
        "    acting\n",
        "    \"\"\"\n",
        "    if random.uniform(0,1)<=self.epsilon:\n",
        "      return env.action_space.sample()\n",
        "    else:\n",
        "      act_values = self.model.predict(state)\n",
        "      return np.argmax(act_values[0])\n",
        "\n",
        "\n",
        "  def replay(self,batch_size):\n",
        "    # training\n",
        "    if len(self.memory) < batch_size:\n",
        "      return\n",
        "    minibatch = random.sample(self.memory,batch_size)\n",
        "\n",
        "    for (state,action,reward,next_state,done) in minibatch:\n",
        "      if done:\n",
        "        target = reward\n",
        "      else:\n",
        "        target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\n",
        "\n",
        "      train_target = self.model.predict(state)\n",
        "      train_target[0][action] = target\n",
        "\n",
        "      self.model.fit(state, train_target,verbose=0)\n",
        "\n",
        "\n",
        "  def adaptiveEGreedy(self):\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "      self.epsilon *= self.epsilon_decay\n",
        "  \n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  # initilaize env and agent\n",
        "  \n",
        "  episode=100\n",
        "  batch_size=16\n",
        "  env = gym.make('CartPole-v0')\n",
        "  agent = DQLAgent(env)\n",
        "\n",
        "  for e in range(episode):\n",
        "    # init env\n",
        "    state = env.reset()\n",
        "    state=np.reshape(state,[1,4])\n",
        "    time = 0\n",
        "    while True:\n",
        "      #act\n",
        "      action = agent.act(state)\n",
        "      #step\n",
        "      next_state,reward,done,_ = env.step(action)\n",
        "      next_state=np.reshape(next_state,[1,4])\n",
        "\n",
        "      #remember\n",
        "      agent.remember(state,action,reward,next_state,done)\n",
        "      #update stage\n",
        "      state = next_state\n",
        "      #replay\n",
        "      agent.replay(batch_size)\n",
        "      #adjust epsilon\n",
        "      agent.adaptiveEGreedy()\n",
        "\n",
        "      time+=1\n",
        "\n",
        "      if done:\n",
        "        print(\"Episode: {}, time: {}\".format(e,time))\n",
        "        break"
      ],
      "metadata": {
        "id": "TnmsgJbBod01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model=agent\n",
        "state = env.reset()\n",
        "state = np.reshape(state,[1,4])\n",
        "time_t=0\n",
        "\n",
        "while True:\n",
        "  env.render()\n",
        "  trained_model.act(state)\n",
        "  next_state, reward, done, _ = env.step(action)\n",
        "  next_state = np.reshape(next_state,[1,4])\n",
        "  state = next_state\n",
        "  time_t +=1\n",
        "  print(time)\n",
        "  time.sleep(0.4)\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "print(\"Done !\")"
      ],
      "metadata": {
        "id": "u1vteEKttTii"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}